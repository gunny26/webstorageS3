#!/usr/bin/python3
# pylint: disable=line-too-long
"""
RestFUL Webclient to use FileStorage and BlockStorage WebApps
"""
import json
import logging
import gzip
import hashlib
from io import BytesIO
# own modules
from .StorageClientS3 import StorageClient

class WebStorageArchiveClient(StorageClient):
    """
    store and retrieve Data, specific for WebStorageArchives
    """
    def __init__(self):
        """__init__"""
        super(WebStorageArchiveClient, self).__init__()
        self._logger = logging.getLogger(self.__class__.__name__)
        self._bucket_name = self._config["WEBSTORAGE_BUCKET_NAME"]
        self._logger.debug("bucket list: %s", self._client.list_buckets())

    def get_backupsets(self, hostname=None):
        """
        get all available backupsets
        works like directory listing of *.wstar.gz
        returns data sorted by datetime of filename

        :param hostname <str>: if not given use local hostname
        :return <list>: list of all stored backupsets for this hostname
        """
        result = {}
        for key in self._list_objects():  # get keys in bucket
            response = self._client.head_object(Bucket=self._bucket_name, Key=key)  # TODO: exceptions
            size = response['ContentLength']
            if "Metadata" in response and response["Metadata"]:
                thishostname = response["Metadata"]["hostname"]
                tag = response["Metadata"]["tag"]
                timestamp = response["Metadata"]["datetime"]
                # 2016-10-25T20:23:17.782902
                thisdate, thistime = timestamp.split("T")
                thistime = thistime.split(".")[0]
                if hostname and hostname != thishostname:  # filter only backupsets for this hostname
                    continue
                result[key] = {
                    "date": thisdate,
                    "time": thistime,
                    "datetime": timestamp,
                    "size": size,
                    "tag": tag,
                    "hostname": thishostname,
                    "basename": key
                }
        # sort by datetime
        return sorted(result.values(), key=lambda a: a["datetime"])

    def get_latest_backupset(self, hostname):
        """
        get the latest backupset stored shorthand function to get_backupsets

        :param hostname <str>: hsotname of client
        :returns <str>: filename of latest stored backupset for this hostname
        """
        try:
            return self.get_backupsets(hostname)[-1]["basename"]
        except IndexError:
            pass

    def read(self, filename):
        """
        read content of stored WebstorageArchive

        :param filename <str>: filename of archive, will base64 encoded
        :return <dict>: metadata of archive
        """
        b_buffer = BytesIO()
        self._client.download_fileobj(self._bucket_name, filename, b_buffer) # TODO: exceptions
        b_buffer.seek(0) # do not forget this tiny little line !!
        gzip_data = b_buffer.read()
        data = self._gunzip_bytes(gzip_data)
        return json.loads(data)

    def _gzip_str(self, data):
        """
        gzip some string and return bytes

        :param data <str>: some string, will be utf-8 encoded
        """
        out = BytesIO()
        with gzip.GzipFile(fileobj=out, mode='w') as outfile:
            outfile.write(data.encode("utf-8"))
        out.seek(0)
        return out

    def _gunzip_bytes(self, bytes_obj):
        """
        gunzip some bytes data to string

        :param bytes_obj <bytes>: as returned by _gzip_str
        :returns <str>:
        """
        in_ = BytesIO()
        in_.write(bytes_obj)
        in_.seek(0)
        with gzip.GzipFile(fileobj=in_, mode='rb') as infile:
            gunzipped_bytes_obj = infile.read()
        return gunzipped_bytes_obj.decode("utf-8")

    def save(self, data):
        """
        save data generated by wstar on webstorage
        data should be some json encodable python object
        will be encoded in utf-8 before building sha256 checksum

        :param data <dict>: information about stored files and directories
        """
       # build sha256 checksum of data
        sha256 = hashlib.sha256() # TODO: put this in config ?
        sha256.update(json.dumps(data, sort_keys=True).encode("utf-8"))
        data["checksum"] = sha256.hexdigest()
        self._logger.info("checksum of archive %s", data["checksum"])
        # store
        extra_args = {
            "Metadata": {
                "hostname" : data["hostname"],
                "tag": data["tag"],
                "datetime": data["datetime"]
            }
        }
        # f_object = BytesIO(json.dumps(data).encode("utf-8"))
        f_object = self._gzip_str(json.dumps(data))
        self._client.upload_fileobj(f_object, self._bucket_name, data["checksum"], ExtraArgs=extra_args)
